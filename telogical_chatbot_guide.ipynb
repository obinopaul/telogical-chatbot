{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telogical Chatbot Framework Guide\n",
    "\n",
    "This notebook provides a comprehensive guide to understanding the Telogical Chatbot framework, its architecture, and how to integrate your own AI agents into it.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Project Overview](#project-overview)\n",
    "2. [Architecture](#architecture)\n",
    "3. [AI Agent Integration](#ai-agent-integration)\n",
    "4. [Service Layer (FastAPI)](#service-layer)\n",
    "5. [Client Integration](#client-integration)\n",
    "6. [Front-end Options](#frontend-options)\n",
    "7. [Docker Deployment](#docker-deployment)\n",
    "8. [RAG Integration](#rag-integration)\n",
    "9. [Custom Tool Visualization](#custom-tool-visualization)\n",
    "10. [End-to-End Examples](#end-to-end-examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Overview <a name=\"project-overview\"></a>\n",
    "\n",
    "The Telogical Chatbot framework is a comprehensive toolkit for building, deploying, and managing AI agents. It provides:\n",
    "\n",
    "- A modular architecture for integrating various types of AI agents\n",
    "- A service layer built with FastAPI for exposing agents via API\n",
    "- Client libraries for interacting with the service\n",
    "- A Streamlit-based UI for demonstration and interaction\n",
    "- Docker containerization for easy deployment\n",
    "- Support for various memory backends (MongoDB, PostgreSQL, SQLite)\n",
    "- Integration with LangGraph for complex agent workflows\n",
    "\n",
    "The framework is designed to be flexible, allowing you to use components individually or together as a complete solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture <a name=\"architecture\"></a>\n",
    "\n",
    "The project follows a modular architecture with several key components:\n",
    "\n",
    "### Core Components:\n",
    "\n",
    "```\n",
    "┌───────────────┐      ┌────────────────┐      ┌────────────────┐\n",
    "│   AI Agents   │◄─────┤  Service Layer │◄─────┤  Client Layer  │\n",
    "│  (LLM-based)  │      │   (FastAPI)    │      │                │\n",
    "└───────┬───────┘      └────────────────┘      └────────────────┘\n",
    "        │                                             ▲\n",
    "        │                                             │\n",
    "        ▼                                             │\n",
    "┌───────────────┐                           ┌─────────────────┐\n",
    "│    Memory     │                           │   Front-end     │\n",
    "│   Backends    │                           │  (Streamlit)    │\n",
    "└───────────────┘                           └─────────────────┘\n",
    "```\n",
    "\n",
    "### Key Directories and Files:\n",
    "\n",
    "- **src/agents/**: Contains all AI agent implementations\n",
    "- **src/service/**: FastAPI service layer\n",
    "- **src/client/**: Client library for interacting with the service\n",
    "- **src/core/**: Core settings and LLM interfaces\n",
    "- **src/memory/**: Memory backends for conversation history\n",
    "- **src/schema/**: Data models and schemas\n",
    "- **src/streamlit_app.py**: Streamlit front-end\n",
    "- **docker/**: Docker configuration files\n",
    "\n",
    "### Key Flows:\n",
    "\n",
    "1. **Front-end → Client → Service → Agent**: User input flows from the UI through the client to the service to the agent\n",
    "2. **Agent → Memory**: Conversations are stored in the selected memory backend\n",
    "3. **Agent → Tools**: Agents can use various tools to perform tasks\n",
    "4. **Service → Client → Front-end**: Agent responses flow back to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AI Agent Integration <a name=\"ai-agent-integration\"></a>\n",
    "\n",
    "The framework supports multiple types of agents, which are implemented in the `src/agents/` directory. Let's explore how to integrate your own agent:\n",
    "\n",
    "### Agent Structure\n",
    "\n",
    "Agents in this framework are built on the concept of a base agent class that handles common functionality, with specific agent types implementing their unique behavior. The main agent types include:\n",
    "\n",
    "- **Chatbot** (`src/agents/chatbot.py`): Basic conversational agent\n",
    "- **RAG Assistant** (`src/agents/rag_assistant.py`): Retrieval-augmented generation agent\n",
    "- **Research Assistant** (`src/agents/research_assistant.py`): Web search capable agent\n",
    "- **LangGraph Supervisor** (`src/agents/langgraph_supervisor_agent.py`): Complex workflow agent\n",
    "\n",
    "### Integrating Your Own Agent\n",
    "\n",
    "To integrate your own agent, follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating a custom agent\n",
    "from src.agents.agents import BaseAgent\n",
    "from src.schema.models import ChatMessage, MessageRole\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "class MyCustomAgent(BaseAgent):\n",
    "    \"\"\"Custom agent implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, **kwargs):\n",
    "        super().__init__(model_name=model_name, **kwargs)\n",
    "        # Initialize your custom components here\n",
    "        self.my_custom_component = None\n",
    "    \n",
    "    async def process_message(\n",
    "        self, \n",
    "        messages: List[ChatMessage], \n",
    "        stream: bool = False,\n",
    "        **kwargs\n",
    "    ) -> ChatMessage:\n",
    "        \"\"\"Process a message from the user\"\"\"\n",
    "        # Your custom logic here\n",
    "        # Example: Prepare messages for the LLM\n",
    "        prepared_messages = self._prepare_messages(messages)\n",
    "        \n",
    "        # Call the LLM\n",
    "        if stream:\n",
    "            return await self._stream_llm_response(prepared_messages, **kwargs)\n",
    "        else:\n",
    "            return await self._get_llm_response(prepared_messages, **kwargs)\n",
    "    \n",
    "    def _prepare_messages(self, messages: List[ChatMessage]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Prepare messages for the LLM\"\"\"\n",
    "        # Your custom message preparation logic\n",
    "        return [{\n",
    "            \"role\": msg.role.value,\n",
    "            \"content\": msg.content\n",
    "        } for msg in messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering Your Agent\n",
    "\n",
    "After creating your agent class, you need to register it with the framework so it can be instantiated through the service layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register your agent in src/agents/agents.py\n",
    "from src.agents.agents import AGENT_REGISTRY\n",
    "\n",
    "# Add your agent to the registry\n",
    "AGENT_REGISTRY[\"my_custom_agent\"] = MyCustomAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Tool Support\n",
    "\n",
    "If your agent needs to use tools, you can add tool support like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.tools import BaseTool\n",
    "\n",
    "# Create a custom tool\n",
    "class MyCustomTool(BaseTool):\n",
    "    name = \"my_custom_tool\"\n",
    "    description = \"A tool that does something useful\"\n",
    "    \n",
    "    def _run(self, input_text: str) -> str:\n",
    "        \"\"\"Execute the tool\"\"\"\n",
    "        # Your tool logic here\n",
    "        return f\"Processed: {input_text}\"\n",
    "\n",
    "# Then in your agent class:\n",
    "def __init__(self, model_name: str, **kwargs):\n",
    "    super().__init__(model_name=model_name, **kwargs)\n",
    "    # Add your custom tool to the agent\n",
    "    self.tools = [MyCustomTool()]\n",
    "    \n",
    "    # Configure your agent to use tools\n",
    "    self.tool_config = {\n",
    "        \"tools\": [tool.to_dict() for tool in self.tools],\n",
    "        \"tool_choice\": \"auto\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Tool Execution\n",
    "\n",
    "To visualize tool execution without exposing the actual tools to the user, you can leverage the framework's message tracing capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adding tool execution tracing to your agent\n",
    "from src.schema.models import ChatMessage, MessageRole, MessageMetadata\n",
    "\n",
    "async def process_message(self, messages: List[ChatMessage], **kwargs):\n",
    "    # ... your existing code ...\n",
    "    \n",
    "    # When a tool is executed, add trace information\n",
    "    tool_name = \"my_tool\"\n",
    "    tool_input = \"some input\"\n",
    "    tool_output = \"some output\"\n",
    "    \n",
    "    # Create a metadata object with trace information\n",
    "    metadata = MessageMetadata(\n",
    "        trace={\n",
    "            \"tool_calls\": [{\n",
    "                \"name\": tool_name,\n",
    "                \"input\": tool_input,\n",
    "                \"output\": tool_output\n",
    "            }]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add the metadata to the response message\n",
    "    response = ChatMessage(\n",
    "        role=MessageRole.ASSISTANT,\n",
    "        content=\"I've analyzed the data.\",\n",
    "        metadata=metadata\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Service Layer (FastAPI) <a name=\"service-layer\"></a>\n",
    "\n",
    "The service layer is implemented using FastAPI and provides REST and WebSocket endpoints for interacting with agents. Let's explore how to understand and extend the service layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service Architecture\n",
    "\n",
    "The service is defined in `src/service/service.py` and provides these key endpoints:\n",
    "\n",
    "- **POST /chat**: Send a message to an agent and get a response\n",
    "- **WebSocket /ws/chat**: Stream messages to and from an agent\n",
    "- **GET /agents**: List available agents\n",
    "- **GET /health**: Health check endpoint\n",
    "\n",
    "### Running the Service\n",
    "\n",
    "The service can be started using the `src/run_service.py` script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the service\n",
    "!python src/run_service.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the Service\n",
    "\n",
    "To add new endpoints or functionality to the service, you can extend the `service.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adding a new endpoint to the service\n",
    "from fastapi import APIRouter, Depends, HTTPException\n",
    "from src.schema.models import ChatRequest, ChatResponse\n",
    "from src.service.utils import get_agent_instance\n",
    "\n",
    "# Create a new router (in service.py)\n",
    "custom_router = APIRouter(prefix=\"/custom\", tags=[\"custom\"])\n",
    "\n",
    "@custom_router.post(\"/analyze\")\n",
    "async def analyze_data(request: ChatRequest):\n",
    "    \"\"\"Custom endpoint for data analysis\"\"\"\n",
    "    try:\n",
    "        # Get the agent instance\n",
    "        agent = get_agent_instance(request.agent_id, request.agent_type)\n",
    "        \n",
    "        # Process the request\n",
    "        messages = request.messages\n",
    "        # Add a system message indicating this is an analysis request\n",
    "        messages.insert(0, ChatMessage(\n",
    "            role=MessageRole.SYSTEM,\n",
    "            content=\"This is a data analysis request. Focus on extracting insights.\"\n",
    "        ))\n",
    "        \n",
    "        # Get the response\n",
    "        response = await agent.process_message(messages)\n",
    "        \n",
    "        return ChatResponse(message=response)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Add the router to the main app\n",
    "app.include_router(custom_router)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Client Integration <a name=\"client-integration\"></a>\n",
    "\n",
    "The client library (`src/client/client.py`) provides a convenient way to interact with the service layer. Here's how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using the client library\n",
    "from src.client.client import TelogicalClient\n",
    "from src.schema.models import ChatMessage, MessageRole\n",
    "\n",
    "# Create a client instance\n",
    "client = TelogicalClient(base_url=\"http://localhost:8000\")\n",
    "\n",
    "# Send a message to an agent\n",
    "async def chat_with_agent():\n",
    "    message = ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"What is machine learning?\"\n",
    "    )\n",
    "    \n",
    "    response = await client.chat(\n",
    "        agent_type=\"chatbot\",\n",
    "        messages=[message]\n",
    "    )\n",
    "    \n",
    "    print(response.content)\n",
    "\n",
    "# Stream messages from an agent\n",
    "async def stream_from_agent():\n",
    "    message = ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"Explain quantum computing in simple terms.\"\n",
    "    )\n",
    "    \n",
    "    async for chunk in client.chat_stream(\n",
    "        agent_type=\"chatbot\",\n",
    "        messages=[message]\n",
    "    ):\n",
    "        print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the Client\n",
    "\n",
    "You can extend the client to add support for your custom endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extending the client for custom endpoints\n",
    "class ExtendedClient(TelogicalClient):\n",
    "    \"\"\"Extended client with custom functionality\"\"\"\n",
    "    \n",
    "    async def analyze_data(self, messages):\n",
    "        \"\"\"Call the custom analyze endpoint\"\"\"\n",
    "        url = f\"{self.base_url}/custom/analyze\"\n",
    "        payload = {\n",
    "            \"agent_type\": \"my_custom_agent\",\n",
    "            \"messages\": [msg.dict() for msg in messages]\n",
    "        }\n",
    "        \n",
    "        async with self.session.post(url, json=payload) as response:\n",
    "            response.raise_for_status()\n",
    "            data = await response.json()\n",
    "            return ChatMessage(**data[\"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Front-end Options <a name=\"frontend-options\"></a>\n",
    "\n",
    "The framework includes a Streamlit front-end (`src/streamlit_app.py`), but you can integrate with other front-end technologies like React or Vue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Streamlit Front-end\n",
    "\n",
    "The Streamlit app provides a simple interface for interacting with agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Streamlit app\n",
    "!streamlit run src/streamlit_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating with React\n",
    "\n",
    "To integrate with a React front-end, you would create a React application that communicates with the FastAPI service. Here's an example of how to create a simple React client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile react-client-example.jsx\n",
    "// Example React component for chatting with the agent\n",
    "import React, { useState, useEffect, useRef } from 'react';\n",
    "import axios from 'axios';\n",
    "\n",
    "const ChatComponent = () => {\n",
    "  const [messages, setMessages] = useState([]);\n",
    "  const [input, setInput] = useState('');\n",
    "  const [loading, setLoading] = useState(false);\n",
    "  const [traceInfo, setTraceInfo] = useState(null);\n",
    "  \n",
    "  const sendMessage = async () => {\n",
    "    if (!input.trim()) return;\n",
    "    \n",
    "    // Add user message to chat\n",
    "    const userMessage = { role: 'user', content: input };\n",
    "    setMessages([...messages, userMessage]);\n",
    "    setInput('');\n",
    "    setLoading(true);\n",
    "    \n",
    "    try {\n",
    "      // Send message to API\n",
    "      const response = await axios.post('http://localhost:8000/chat', {\n",
    "        agent_type: 'chatbot',\n",
    "        messages: [...messages, userMessage].map(msg => ({\n",
    "          role: msg.role,\n",
    "          content: msg.content\n",
    "        }))\n",
    "      });\n",
    "      \n",
    "      // Add assistant response to chat\n",
    "      const assistantMessage = response.data.message;\n",
    "      setMessages([...messages, userMessage, assistantMessage]);\n",
    "      \n",
    "      // Check for trace information\n",
    "      if (assistantMessage.metadata?.trace) {\n",
    "        setTraceInfo(assistantMessage.metadata.trace);\n",
    "      }\n",
    "    } catch (error) {\n",
    "      console.error('Error sending message:', error);\n",
    "    } finally {\n",
    "      setLoading(false);\n",
    "    }\n",
    "  };\n",
    "  \n",
    "  return (\n",
    "    <div className=\"chat-container\">\n",
    "      <div className=\"messages-container\">\n",
    "        {messages.map((msg, index) => (\n",
    "          <div key={index} className={`message ${msg.role}`}>\n",
    "            <div className=\"message-content\">{msg.content}</div>\n",
    "          </div>\n",
    "        ))}\n",
    "        {loading && <div className=\"loading\">AI is thinking...</div>}\n",
    "      </div>\n",
    "      \n",
    "      {traceInfo && (\n",
    "        <div className=\"trace-info\">\n",
    "          <h3>Agent Actions</h3>\n",
    "          {traceInfo.tool_calls?.map((tool, index) => (\n",
    "            <div key={index} className=\"tool-call\">\n",
    "              <div className=\"tool-name\">Using tool: {tool.name}</div>\n",
    "              <div className=\"tool-input\">Input: {tool.input}</div>\n",
    "              <div className=\"tool-output\">Result: {tool.output}</div>\n",
    "            </div>\n",
    "          ))}\n",
    "        </div>\n",
    "      )}\n",
    "      \n",
    "      <div className=\"input-container\">\n",
    "        <input\n",
    "          type=\"text\"\n",
    "          value={input}\n",
    "          onChange={(e) => setInput(e.target.value)}\n",
    "          onKeyPress={(e) => e.key === 'Enter' && sendMessage()}\n",
    "          placeholder=\"Type your message...\"\n",
    "          disabled={loading}\n",
    "        />\n",
    "        <button onClick={sendMessage} disabled={loading}>\n",
    "          Send\n",
    "        </button>\n",
    "      </div>\n",
    "    </div>\n",
    "  );\n",
    "};\n",
    "\n",
    "export default ChatComponent;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating with Node.js\n",
    "\n",
    "For a Node.js backend that communicates with the FastAPI service, you could create an Express server that acts as a proxy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile nodejs-proxy-example.js\n",
    "// Example Node.js Express server as a proxy to the FastAPI service\n",
    "const express = require('express');\n",
    "const axios = require('axios');\n",
    "const cors = require('cors');\n",
    "const app = express();\n",
    "const port = 3000;\n",
    "\n",
    "// Enable CORS and JSON parsing\n",
    "app.use(cors());\n",
    "app.use(express.json());\n",
    "\n",
    "// Base URL for the FastAPI service\n",
    "const API_BASE_URL = 'http://localhost:8000';\n",
    "\n",
    "// Proxy endpoint for chat\n",
    "app.post('/api/chat', async (req, res) => {\n",
    "  try {\n",
    "    const response = await axios.post(`${API_BASE_URL}/chat`, req.body);\n",
    "    res.json(response.data);\n",
    "  } catch (error) {\n",
    "    console.error('Error proxying request:', error);\n",
    "    res.status(error.response?.status || 500).json({\n",
    "      error: error.response?.data || 'Internal server error'\n",
    "    });\n",
    "  }\n",
    "});\n",
    "\n",
    "// Proxy endpoint for listing agents\n",
    "app.get('/api/agents', async (req, res) => {\n",
    "  try {\n",
    "    const response = await axios.get(`${API_BASE_URL}/agents`);\n",
    "    res.json(response.data);\n",
    "  } catch (error) {\n",
    "    console.error('Error proxying request:', error);\n",
    "    res.status(error.response?.status || 500).json({\n",
    "      error: error.response?.data || 'Internal server error'\n",
    "    });\n",
    "  }\n",
    "});\n",
    "\n",
    "// Serve static files (for the React front-end)\n",
    "app.use(express.static('public'));\n",
    "\n",
    "// Start the server\n",
    "app.listen(port, () => {\n",
    "  console.log(`Node.js proxy server listening on port ${port}`);\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Docker Deployment <a name=\"docker-deployment\"></a>\n",
    "\n",
    "The framework includes Docker configuration for easy deployment. Let's explore how to use it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Compose Setup\n",
    "\n",
    "The project includes a `compose.yaml` file that defines the services needed to run the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the application using Docker Compose\n",
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Custom Docker Images\n",
    "\n",
    "If you need to customize the Docker images, you can modify the Dockerfiles in the `docker/` directory and build your own images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a custom Docker image for the service\n",
    "!docker build -f docker/Dockerfile.service -t my-custom-telogical-service ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying to a Cloud Provider\n",
    "\n",
    "You can deploy the Docker containers to various cloud providers like AWS, GCP, or Azure. Here's an example for AWS ECS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile deploy-to-ecs.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# Build and push Docker images to ECR\n",
    "aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com\n",
    "\n",
    "# Build and push service image\n",
    "docker build -f docker/Dockerfile.service -t telogical-service .\n",
    "docker tag telogical-service:latest $AWS_ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/telogical-service:latest\n",
    "docker push $AWS_ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/telogical-service:latest\n",
    "\n",
    "# Update ECS service\n",
    "aws ecs update-service --cluster telogical-cluster --service telogical-service --force-new-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RAG Integration <a name=\"rag-integration\"></a>\n",
    "\n",
    "The framework includes support for Retrieval-Augmented Generation (RAG) via the `rag_assistant.py` agent. Let's explore how to use and customize it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up a Vector Database\n",
    "\n",
    "The framework includes a script for creating a Chroma vector database from documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script to create a Chroma database\n",
    "!python scripts/create_chroma_db.py --input_dir /path/to/documents --output_dir data/chroma_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the RAG Assistant\n",
    "\n",
    "You can customize the RAG assistant by extending the `RAGAssistant` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Customizing the RAG Assistant\n",
    "from src.agents.rag_assistant import RAGAssistant\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "class CustomRAGAssistant(RAGAssistant):\n",
    "    \"\"\"Custom RAG assistant with specialized retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, **kwargs):\n",
    "        super().__init__(model_name=model_name, **kwargs)\n",
    "        \n",
    "        # Initialize custom embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        \n",
    "        # Initialize vector store with custom settings\n",
    "        self.vectorstore = Chroma(\n",
    "            collection_name=\"custom_docs\",\n",
    "            embedding_function=embeddings,\n",
    "            persist_directory=\"data/custom_chroma_db\"\n",
    "        )\n",
    "        \n",
    "        # Customize retrieval parameters\n",
    "        self.top_k = 5\n",
    "        self.similarity_threshold = 0.7\n",
    "    \n",
    "    async def _retrieve_relevant_documents(self, query: str):\n",
    "        \"\"\"Custom document retrieval logic\"\"\"\n",
    "        # Add your custom retrieval logic here\n",
    "        docs = self.vectorstore.similarity_search_with_score(\n",
    "            query, k=self.top_k\n",
    "        )\n",
    "        \n",
    "        # Filter by similarity threshold\n",
    "        filtered_docs = [doc for doc, score in docs if score >= self.similarity_threshold]\n",
    "        \n",
    "        return filtered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Tool Visualization <a name=\"custom-tool-visualization\"></a>\n",
    "\n",
    "To visualize the agent's reasoning process and tool usage without exposing the actual tools to the user, you can implement a custom visualization in your front-end:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Trace Information to Messages\n",
    "\n",
    "When your agent uses tools, it can add trace information to the response messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adding trace information to messages\n",
    "from src.schema.models import ChatMessage, MessageRole, MessageMetadata\n",
    "\n",
    "def create_response_with_trace(content: str, tool_calls: list):\n",
    "    \"\"\"Create a response message with trace information\"\"\"\n",
    "    metadata = MessageMetadata(\n",
    "        trace={\n",
    "            \"tool_calls\": tool_calls,\n",
    "            \"reasoning\": \"I need to search for information about quantum computing.\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return ChatMessage(\n",
    "        role=MessageRole.ASSISTANT,\n",
    "        content=content,\n",
    "        metadata=metadata\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Trace Information in the UI\n",
    "\n",
    "You can then display this trace information in your UI, whether it's Streamlit, React, or another framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile streamlit-trace-visualization-example.py\n",
    "import streamlit as st\n",
    "from src.client.client import TelogicalClient\n",
    "from src.schema.models import ChatMessage, MessageRole\n",
    "\n",
    "# Initialize client\n",
    "client = TelogicalClient(base_url=\"http://localhost:8000\")\n",
    "\n",
    "# Initialize session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message.role.value):\n",
    "        st.write(message.content)\n",
    "        \n",
    "        # Display trace information if available\n",
    "        if message.metadata and message.metadata.trace:\n",
    "            with st.expander(\"Agent Reasoning\"):\n",
    "                st.write(message.metadata.trace.get(\"reasoning\", \"\"))\n",
    "            \n",
    "            if \"tool_calls\" in message.metadata.trace:\n",
    "                for i, tool_call in enumerate(message.metadata.trace[\"tool_calls\"]):\n",
    "                    with st.expander(f\"Tool: {tool_call['name']}\"):\n",
    "                        st.write(f\"**Input:** {tool_call['input']}\")\n",
    "                        st.write(f\"**Output:** {tool_call['output']}\")\n",
    "\n",
    "# Chat input\n",
    "if prompt := st.chat_input():\n",
    "    # Add user message to chat\n",
    "    user_message = ChatMessage(role=MessageRole.USER, content=prompt)\n",
    "    st.session_state.messages.append(user_message)\n",
    "    \n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(prompt)\n",
    "    \n",
    "    # Get response from agent\n",
    "    with st.spinner(\"Thinking...\"):\n",
    "        response = await client.chat(\n",
    "            agent_type=\"research_assistant\",  # This agent uses tools\n",
    "            messages=st.session_state.messages\n",
    "        )\n",
    "        \n",
    "        st.session_state.messages.append(response)\n",
    "    \n",
    "    # Display assistant response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.write(response.content)\n",
    "        \n",
    "        # Display trace information\n",
    "        if response.metadata and response.metadata.trace:\n",
    "            with st.expander(\"Agent Reasoning\"):\n",
    "                st.write(response.metadata.trace.get(\"reasoning\", \"\"))\n",
    "            \n",
    "            if \"tool_calls\" in response.metadata.trace:\n",
    "                for i, tool_call in enumerate(response.metadata.trace[\"tool_calls\"]):\n",
    "                    with st.expander(f\"Tool: {tool_call['name']}\"):\n",
    "                        st.write(f\"**Input:** {tool_call['input']}\")\n",
    "                        st.write(f\"**Output:** {tool_call['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. End-to-End Examples <a name=\"end-to-end-examples\"></a>\n",
    "\n",
    "Let's put everything together with an end-to-end example of integrating a custom agent, connecting it to the service, and building a front-end to interact with it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a Custom Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/agents/my_reasoning_agent.py\n",
    "from typing import List, Dict, Any, Optional\n",
    "from src.agents.agents import BaseAgent\n",
    "from src.schema.models import ChatMessage, MessageRole, MessageMetadata\n",
    "from src.agents.tools import BaseTool\n",
    "\n",
    "class CalculatorTool(BaseTool):\n",
    "    \"\"\"A simple calculator tool\"\"\"\n",
    "    name = \"calculator\"\n",
    "    description = \"Evaluate mathematical expressions\"\n",
    "    \n",
    "    def _run(self, input_text: str) -> str:\n",
    "        \"\"\"Evaluate a mathematical expression\"\"\"\n",
    "        try:\n",
    "            # Security: Use safe evaluation to prevent code execution\n",
    "            result = eval(input_text, {\"__builtins__\": {}}, {})\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "class ReasoningAgent(BaseAgent):\n",
    "    \"\"\"Custom agent that shows its reasoning\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, **kwargs):\n",
    "        super().__init__(model_name=model_name, **kwargs)\n",
    "        \n",
    "        # Initialize tools\n",
    "        self.tools = [CalculatorTool()]\n",
    "        \n",
    "        # Configure the agent to use tools\n",
    "        self.tool_config = {\n",
    "            \"tools\": [tool.to_dict() for tool in self.tools],\n",
    "            \"tool_choice\": \"auto\"\n",
    "        }\n",
    "    \n",
    "    async def process_message(\n",
    "        self, \n",
    "        messages: List[ChatMessage], \n",
    "        stream: bool = False,\n",
    "        **kwargs\n",
    "    ) -> ChatMessage:\n",
    "        \"\"\"Process a message with visible reasoning\"\"\"\n",
    "        # Prepare system message to guide the agent's behavior\n",
    "        system_message = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a reasoning agent that explains your thought process. \"\n",
    "                \"First, analyze the problem and explain your reasoning step by step. \"\n",
    "                \"If calculations are needed, use the calculator tool. \"\n",
    "                \"After using tools, provide a final answer.\"\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Prepare messages for the LLM\n",
    "        prepared_messages = [system_message] + [{\n",
    "            \"role\": msg.role.value,\n",
    "            \"content\": msg.content\n",
    "        } for msg in messages]\n",
    "        \n",
    "        # Call the LLM with tool support\n",
    "        llm_response = await self.llm.chat_with_tools(\n",
    "            messages=prepared_messages,\n",
    "            tools=self.tool_config[\"tools\"],\n",
    "            tool_choice=self.tool_config[\"tool_choice\"],\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        # Extract the response content and tool calls\n",
    "        content = llm_response[\"content\"] if llm_response[\"content\"] else \"\"\n",
    "        tool_calls = []\n",
    "        \n",
    "        # Process tool calls if any\n",
    "        if \"tool_calls\" in llm_response:\n",
    "            for tool_call in llm_response[\"tool_calls\"]:\n",
    "                tool_name = tool_call[\"function\"][\"name\"]\n",
    "                tool_input = tool_call[\"function\"][\"arguments\"]\n",
    "                \n",
    "                # Find the tool\n",
    "                tool = next((t for t in self.tools if t.name == tool_name), None)\n",
    "                \n",
    "                if tool:\n",
    "                    # Execute the tool\n",
    "                    tool_output = tool.run(tool_input)\n",
    "                    \n",
    "                    # Add to tool calls for tracing\n",
    "                    tool_calls.append({\n",
    "                        \"name\": tool_name,\n",
    "                        \"input\": tool_input,\n",
    "                        \"output\": tool_output\n",
    "                    })\n",
    "        \n",
    "        # Create a metadata object with trace information\n",
    "        metadata = MessageMetadata(\n",
    "            trace={\n",
    "                \"tool_calls\": tool_calls,\n",
    "                \"reasoning\": \"Here's my step-by-step reasoning: ...\"  # You could extract this from the content\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Return the response message\n",
    "        return ChatMessage(\n",
    "            role=MessageRole.ASSISTANT,\n",
    "            content=content,\n",
    "            metadata=metadata\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Register the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/agents/agents.py.updated\n",
    "# Add this to the end of the existing agents.py file\n",
    "from src.agents.my_reasoning_agent import ReasoningAgent\n",
    "\n",
    "# Register the new agent\n",
    "AGENT_REGISTRY[\"reasoning_agent\"] = ReasoningAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a Front-end to Visualize the Agent's Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/reasoning_app.py\n",
    "import streamlit as st\n",
    "import asyncio\n",
    "from src.client.client import TelogicalClient\n",
    "from src.schema.models import ChatMessage, MessageRole\n",
    "\n",
    "st.set_page_config(page_title=\"Reasoning Agent\", page_icon=\"🧠\")\n",
    "\n",
    "# Initialize client\n",
    "client = TelogicalClient(base_url=\"http://localhost:8000\")\n",
    "\n",
    "st.title(\"Reasoning Agent Demo\")\n",
    "st.markdown(\n",
    "    \"This demo shows how an AI agent can reveal its reasoning process and tool usage \"\n",
    "    \"without exposing the actual tools to the user.\"\n",
    ")\n",
    "\n",
    "# Initialize session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message.role.value):\n",
    "        st.markdown(message.content)\n",
    "        \n",
    "        # Display reasoning and tool calls if available\n",
    "        if message.role.value == \"assistant\" and message.metadata and message.metadata.trace:\n",
    "            if \"reasoning\" in message.metadata.trace:\n",
    "                with st.expander(\"View Reasoning\"):\n",
    "                    st.markdown(message.metadata.trace[\"reasoning\"])\n",
    "            \n",
    "            if \"tool_calls\" in message.metadata.trace and message.metadata.trace[\"tool_calls\"]:\n",
    "                steps = message.metadata.trace[\"tool_calls\"]\n",
    "                with st.expander(f\"View Steps ({len(steps)} steps)\"):\n",
    "                    for i, step in enumerate(steps):\n",
    "                        st.markdown(f\"**Step {i+1}: Using {step['name']}**\")\n",
    "                        st.markdown(f\"Input: `{step['input']}`\")\n",
    "                        st.markdown(f\"Result: `{step['output']}`\")\n",
    "                        st.divider()\n",
    "\n",
    "# Chat input\n",
    "if prompt := st.chat_input(\"Ask a question...\"):\n",
    "    # Add user message to chat\n",
    "    user_message = ChatMessage(role=MessageRole.USER, content=prompt)\n",
    "    st.session_state.messages.append(user_message)\n",
    "    \n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    # Get response from agent\n",
    "    with st.spinner(\"Thinking...\"):\n",
    "        # Using asyncio.run is not ideal in Streamlit, but this is just for demonstration\n",
    "        async def get_response():\n",
    "            return await client.chat(\n",
    "                agent_type=\"reasoning_agent\",\n",
    "                messages=st.session_state.messages\n",
    "            )\n",
    "        \n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        response = loop.run_until_complete(get_response())\n",
    "        st.session_state.messages.append(response)\n",
    "    \n",
    "    # Display assistant response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.markdown(response.content)\n",
    "        \n",
    "        # Display reasoning and tool calls\n",
    "        if response.metadata and response.metadata.trace:\n",
    "            if \"reasoning\" in response.metadata.trace:\n",
    "                with st.expander(\"View Reasoning\"):\n",
    "                    st.markdown(response.metadata.trace[\"reasoning\"])\n",
    "            \n",
    "            if \"tool_calls\" in response.metadata.trace and response.metadata.trace[\"tool_calls\"]:\n",
    "                steps = response.metadata.trace[\"tool_calls\"]\n",
    "                with st.expander(f\"View Steps ({len(steps)} steps)\"):\n",
    "                    for i, step in enumerate(steps):\n",
    "                        st.markdown(f\"**Step {i+1}: Using {step['name']}**\")\n",
    "                        st.markdown(f\"Input: `{step['input']}`\")\n",
    "                        st.markdown(f\"Result: `{step['output']}`\")\n",
    "                        st.divider()\n",
    "\n",
    "# Add a sidebar with instructions\n",
    "with st.sidebar:\n",
    "    st.header(\"Instructions\")\n",
    "    st.markdown(\n",
    "        \"Ask questions that might require calculations or step-by-step reasoning. \"\n",
    "        \"The agent will show its work.\"\n",
    "    )\n",
    "    st.markdown(\"Example questions:\")\n",
    "    st.markdown(\"- What is 127 * 345?\")\n",
    "    st.markdown(\"- If I have 125 apples and give 37 away, how many do I have left?\")\n",
    "    st.markdown(\"- What's the compound interest on $1000 at 5% for 3 years?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Everything\n",
    "\n",
    "To run the complete system, you would:\n",
    "\n",
    "1. Start the FastAPI service: `python src/run_service.py`\n",
    "2. Run the Streamlit app: `streamlit run src/reasoning_app.py`\n",
    "\n",
    "Alternatively, you can use Docker Compose to start everything:\n",
    "\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "Then access the application at http://localhost:8501 for the Streamlit UI or http://localhost:8000/docs for the FastAPI documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Telogical Chatbot framework provides a comprehensive solution for building, deploying, and managing AI agents. Key takeaways:\n",
    "\n",
    "1. **Modular Architecture**: Components can be used individually or together\n",
    "2. **Flexible Agent Integration**: Easy to add your own custom agents\n",
    "3. **Multiple Front-end Options**: Use Streamlit or integrate with React/Node.js\n",
    "4. **Tool Visualization**: Show agent reasoning without exposing tools\n",
    "5. **Docker Deployment**: Easy containerization and deployment\n",
    "6. **Memory Options**: Multiple backends for conversation history\n",
    "7. **LangGraph Support**: Complex multi-agent workflows\n",
    "\n",
    "To integrate your own agent, focus on:\n",
    "1. Creating an agent class that extends BaseAgent\n",
    "2. Registering it in the AGENT_REGISTRY\n",
    "3. Customizing the front-end to visualize your agent's behavior\n",
    "\n",
    "For deployment, use the provided Docker configuration and adjust as needed for your specific requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
